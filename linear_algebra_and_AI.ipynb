{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjFtA75TQImsbUaHBiKGS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaehoon1/Linear_Algebra_and_AI/blob/main/linear_algebra_and_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import lil_matrix"
      ],
      "metadata": {
        "id": "4zcFoVp8cKap"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mI63fVkuTXQn"
      },
      "outputs": [],
      "source": [
        "def build_vocab(text: str) -> list[str]:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    vocab = sorted(list(set(tokens)))\n",
        "\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V 생성 함수"
      ],
      "metadata": {
        "id": "lq-gAb6bcQnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_corpus(text: str, vocab: list[str]) -> list[list[int]]:\n",
        "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "    sentences = text.lower().split('.')\n",
        "\n",
        "    corpus = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        clean = re.sub(r'[^a-z\\s]', ' ', sentence)\n",
        "        tokens = clean.split()\n",
        "\n",
        "        if not tokens:\n",
        "            continue\n",
        "\n",
        "        token_ids = [word_to_id[token] for token in tokens if token in word_to_id]\n",
        "\n",
        "        if token_ids:\n",
        "            corpus.append(token_ids)\n",
        "\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "6fd8Ig24b0YI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "corpus 생성 함수"
      ],
      "metadata": {
        "id": "G_9QGJoQchbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def cut_file(input_path, output_path, max_lines=1_000_000):\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
        "         open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "\n",
        "        for i, line in enumerate(fin):\n",
        "            if i >= max_lines:\n",
        "                break\n",
        "            fout.write(line)\n",
        "\n",
        "cut_file(\"/content/sample_data/wikisent2.txt\", \"/content/sample_data/wikisent2_cut.txt\")\n",
        "\"\"\"\n",
        "with open(\"/content/sample_data/multiverse.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "vocab = build_vocab(text)\n",
        "corpus = build_corpus(text, vocab)"
      ],
      "metadata": {
        "id": "y9-bs-oUcj9W"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 데이터로부터 V와 corpus 생성"
      ],
      "metadata": {
        "id": "d0byZEv4dvko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_D(corpus, window_size):\n",
        "    D = []\n",
        "    for sentence in corpus:\n",
        "        L = len(sentence)\n",
        "        for i, w in enumerate(sentence):\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(L, i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                D.append((w, sentence[j]))\n",
        "    return D\n"
      ],
      "metadata": {
        "id": "lcxAjumkd7Is"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D 생성 함수"
      ],
      "metadata": {
        "id": "raJPSjfpgHiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_count = Counter(D)\n",
        "w_count = Counter([w for w, _ in D])\n",
        "c_count = Counter([c for _, c in D])\n",
        "D_size = len(D)\n",
        "\n",
        "def p_wc(D, w, c):\n",
        "  return pair_count[(w, c)] / D_size if D_size > 0 else 0.0\n",
        "\n",
        "def p_w(D, w):\n",
        "  return w_count[w] / D_size if D_size > 0 else 0.0\n",
        "\n",
        "#def p_c(D, c):\n",
        "#  return c_count[c] / D_size if D_size > 0 else 0.0\n",
        "# unigram negative sampling distribution: not used"
      ],
      "metadata": {
        "id": "1O1jMmHKgNNL"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "확률 함수"
      ],
      "metadata": {
        "id": "6n5GVXSdh41F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = build_D(corpus, 5)"
      ],
      "metadata": {
        "id": "OcypaBxPoaaW"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D 생성"
      ],
      "metadata": {
        "id": "bSX8KTBtqVs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_counts = Counter([c for _, c in D])\n",
        "\n",
        "c_pow = {c: count ** (3/4) for c, count in c_counts.items()}\n",
        "\n",
        "total = sum(c_pow.values())\n",
        "\n",
        "p_D = {c: value / total for c, value in c_pow.items()}"
      ],
      "metadata": {
        "id": "NGnFTHnWqXaL"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "p_D(c) 확률 분포 정의"
      ],
      "metadata": {
        "id": "NdQW7nqLrYQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_SPPMI_matrix(v, D, p_D, k=10):\n",
        "    count = 0\n",
        "    V = len(v)\n",
        "    SPPMI = np.zeros((V, V), dtype=np.float32)\n",
        "\n",
        "    for w in range(V):\n",
        "        pw = p_w(D, w)\n",
        "        if pw == 0:\n",
        "            continue\n",
        "\n",
        "        for c in range(V):\n",
        "            pwc = p_wc(D, w, c)\n",
        "            if pwc == 0:\n",
        "                continue\n",
        "\n",
        "            pDc = p_D.get(c, 0.0)\n",
        "            if pDc == 0:\n",
        "                continue\n",
        "\n",
        "            # SPPMI = max(log( p(w,c) / (p(w)*k*p_D(c)) ), 0)\n",
        "            sppmi_value = math.log((pwc / (pw * k * pDc)) + 1e-12)\n",
        "\n",
        "            if sppmi_value > 0:\n",
        "                SPPMI[w, c] = sppmi_value\n",
        "            count = count + 1\n",
        "            if count % 1000 == 0:\n",
        "                print(f\"progress: {count} /{V*V}\")\n",
        "\n",
        "    return SPPMI"
      ],
      "metadata": {
        "id": "MrvG91hvrbO0"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shifted Positive PMI 행렬 생성 함수"
      ],
      "metadata": {
        "id": "8hP-eTqhVVCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPPMI = build_SPPMI_matrix(vocab, D, p_D)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7g1ylHqVdKi",
        "outputId": "0a6cf651-f050-421b-d84b-705f1edfd101"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: 1000 /589824\n",
            "progress: 2000 /589824\n",
            "progress: 3000 /589824\n",
            "progress: 4000 /589824\n",
            "progress: 5000 /589824\n",
            "progress: 6000 /589824\n",
            "progress: 7000 /589824\n",
            "progress: 8000 /589824\n",
            "progress: 9000 /589824\n",
            "progress: 10000 /589824\n",
            "progress: 11000 /589824\n",
            "progress: 12000 /589824\n",
            "progress: 13000 /589824\n",
            "progress: 14000 /589824\n",
            "progress: 15000 /589824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shifted Positive PMI 행렬 생성"
      ],
      "metadata": {
        "id": "MQAhV0ksW_Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spmi_to_embedding(SPMI_matrix, k=200):\n",
        "    \"\"\"\n",
        "    입력:\n",
        "      SPMI_matrix : numpy array (V x V)\n",
        "      k : 임베딩 차원\n",
        "\n",
        "    출력:\n",
        "      embeddings : numpy array (V x k)\n",
        "    \"\"\"\n",
        "    svd = TruncatedSVD(n_components=k, n_iter=10, random_state=42)\n",
        "    embeddings = svd.fit_transform(SPMI_matrix)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "Y--7pmiwXGxT"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVD 함수"
      ],
      "metadata": {
        "id": "VAKrmMDlXTfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = spmi_to_embedding(SPPMI)"
      ],
      "metadata": {
        "id": "gJtnf3-wXWsN"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩"
      ],
      "metadata": {
        "id": "41wG-eb6XnAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, vocab, embeddings):\n",
        "    if word not in vocab:\n",
        "        return None\n",
        "    idx = vocab.index(word)\n",
        "    return embeddings[idx]\n",
        "\n",
        "def print_word(vec, vocab, embeddings, top_k=10):\n",
        "    # Normalize input vector\n",
        "    v = vec / (np.linalg.norm(vec) + 1e-9)\n",
        "\n",
        "    # Normalize embeddings matrix\n",
        "    emb_norm = embeddings / (np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-9)\n",
        "\n",
        "    # Cosine similarity 계산\n",
        "    sims = np.dot(emb_norm, v)\n",
        "\n",
        "    # 가장 유사한 top_k 인덱스\n",
        "    top_indices = sims.argsort()[::-1][:top_k]\n",
        "\n",
        "    # 출력\n",
        "    for idx in top_indices:\n",
        "        print(f\"{vocab[idx]}   (sim = {sims[idx]:.4f})\")"
      ],
      "metadata": {
        "id": "bAjrjvegXokG"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터-단어 변환 함수"
      ],
      "metadata": {
        "id": "w333FdijZHuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec1 = get_embedding(\"actual\", vocab, embeddings)\n",
        "vec2 = get_embedding(\"world\", vocab, embeddings)\n",
        "vec3 = vec1 + vec2\n",
        "print_word(vec3, vocab, embeddings, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGR9lhDbZKf1",
        "outputId": "c35cc9f5-f292-43e1-a0cd-78c137bcf90f"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual   (sim = 0.9516)\n",
            "status   (sim = 0.5478)\n",
            "true   (sim = 0.5398)\n",
            "ontological   (sim = 0.5128)\n",
            "whether   (sim = 0.4944)\n",
            "possibility   (sim = 0.4688)\n",
            "equivalent   (sim = 0.4601)\n",
            "description   (sim = 0.4566)\n",
            "modal   (sim = 0.4428)\n",
            "reality   (sim = 0.4399)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "최종 결과"
      ],
      "metadata": {
        "id": "nzyQJQrKaXhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_vec(word, vocab, embeddings):\n",
        "    \"\"\"Return embedding vector of word\"\"\"\n",
        "    idx = vocab.index(word)\n",
        "    return embeddings[idx]\n",
        "\n",
        "\n",
        "def build_sentence_matrix(sentence_tokens, vocab, embeddings):\n",
        "    \"\"\"sentence → embedding matrix (L, d)\"\"\"\n",
        "    vecs = []\n",
        "    for w in sentence_tokens:\n",
        "        if w in vocab:\n",
        "            vecs.append(get_word_vec(w, vocab, embeddings))\n",
        "        else:\n",
        "            # unknown word → zero vector\n",
        "            vecs.append(np.zeros(embeddings.shape[1]))\n",
        "    return np.vstack(vecs)  # (L, d)\n",
        "\n",
        "\n",
        "def self_attention(E):\n",
        "    \"\"\"\n",
        "    E: (L, d)  sentence embedding matrix\n",
        "    Q = E, K = E, V = E (Wq=Wk=Wv=I)\n",
        "    return attention_scores, output\n",
        "    \"\"\"\n",
        "    Q = E\n",
        "    K = E\n",
        "    V = E\n",
        "    d = E.shape[1]\n",
        "\n",
        "    # Attention score (L, L)\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d)\n",
        "\n",
        "    # Softmax row-wise\n",
        "    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
        "    A = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    # Output\n",
        "    O = np.dot(A, V)\n",
        "    return A, O\n",
        "\n",
        "\n",
        "def show_attention(sentence_tokens, attention_matrix, top_k=3):\n",
        "    L = len(sentence_tokens)\n",
        "    for i in range(L):\n",
        "        scores = attention_matrix[i]\n",
        "        top_idx = scores.argsort()[::-1][:top_k]\n",
        "        print(f\"[{sentence_tokens[i]}] → \", end=\"\")\n",
        "        for j in top_idx:\n",
        "            print(f\"{sentence_tokens[j]}({scores[j]:.3f}) \", end=\"\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "9fXW5HNwzKmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"there are many possible worlds in the multiverse\".split()\n",
        "E = build_sentence_matrix(sentence, vocab, embeddings)\n",
        "A, O = self_attention(E)\n",
        "\n",
        "show_attention(sentence, A, top_k=4)"
      ],
      "metadata": {
        "id": "CpAiuposza20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}